{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KteDWmaaFYPy",
        "outputId": "c638ee5b-daca-4c34-b43f-e80a9ae43f48"
      },
      "id": "KteDWmaaFYPy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5b9b25",
      "metadata": {
        "id": "fc5b9b25"
      },
      "outputs": [],
      "source": [
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac59873",
      "metadata": {
        "id": "9ac59873"
      },
      "source": [
        "# Alpaca-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f090ba",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "f0f090ba",
        "outputId": "95df5570-60a6-449e-c97d-2a33fef45d72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6411808ad472>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wxjiao/alpaca-7b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wxjiao/alpaca-7b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m                     \u001b[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 )\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Otherwise we have to be creative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2025\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m             raise OSError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wxjiao/alpaca-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wxjiao/alpaca-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd86548",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "3bd86548",
        "outputId": "ff259fcc-f5e5-4da3-a43e-1af15b00dd0c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-53f5ab30af08>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Encode the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How to travel from Sydney to Hong Kong?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "# Encode the input text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=100, temperature=0.7)\n",
        "\n",
        "# Decode the output text\n",
        "output_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7c2519",
      "metadata": {
        "id": "9e7c2519"
      },
      "source": [
        "# T5-Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c647152",
      "metadata": {
        "id": "2c647152"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Specify the model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30ed37a",
      "metadata": {
        "id": "a30ed37a",
        "outputId": "18b98c1b-0dda-4eff-9f62-652fbbeef9d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Sydney to Hong Kong by train</s>\n",
            "0.29767775535583496\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "# Now you can use the model and tokenizer for your task\n",
        "# For example, let's generate some text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs)\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fdce73",
      "metadata": {
        "id": "a9fdce73"
      },
      "source": [
        "# T5-XXL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd50da7f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "83c149c44417411fa1b500f834dde7ce"
          ]
        },
        "id": "cd50da7f",
        "outputId": "20c80fc0-017f-4fd1-b79c-7d81d32f0acf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83c149c44417411fa1b500f834dde7ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Specify the model\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f26458",
      "metadata": {
        "id": "21f26458",
        "outputId": "48f9ba07-5ddb-425e-e4d1-098fa6a3caab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Take a flight from Sydney to Hong Kong.</s>\n",
            "77.29993033409119\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "# Now you can use the model and tokenizer for your task\n",
        "# For example, let's generate some text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs)\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9189c32",
      "metadata": {
        "id": "a9189c32"
      },
      "source": [
        "# T5-Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64a3274",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "61a02860f2db4b039665012f67d12946",
            "fd72f3dbe6d049a99c9d6bf0eae90dac",
            "616a2675d12b42878689ea38e7efe2fa",
            "f33a5550c0f24f28ab9d32596848c4f3",
            "01f9e173bbcc411b93729fb1b4fc5147",
            "1a81dd2a9e0c4927b0cefdb7ff7b36eb",
            "531de7c13c7945898d4af6b1f7f7036b"
          ]
        },
        "id": "f64a3274",
        "outputId": "4d0531ed-a006-4845-c8a6-1cde1acf7de8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61a02860f2db4b039665012f67d12946",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\phykawing\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd72f3dbe6d049a99c9d6bf0eae90dac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "616a2675d12b42878689ea38e7efe2fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f33a5550c0f24f28ab9d32596848c4f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01f9e173bbcc411b93729fb1b4fc5147",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a81dd2a9e0c4927b0cefdb7ff7b36eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "531de7c13c7945898d4af6b1f7f7036b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Specify the model\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bd85da",
      "metadata": {
        "id": "59bd85da",
        "outputId": "cbbb69e0-0eee-41f9-8754-e87515f2eaac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> The fastest way is by train.</s>\n",
            "0.7222261428833008\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "# Now you can use the model and tokenizer for your task\n",
        "# For example, let's generate some text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs)\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2540dfd8",
      "metadata": {
        "id": "2540dfd8"
      },
      "source": [
        "# Cerebras-GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ed8a80",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "42ed8a80",
        "outputId": "16765690-06b8-4b05-d621-c92f0cd0c7e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\phykawing\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/26.8G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-6.7B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-6.7B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eab38ea",
      "metadata": {
        "id": "6eab38ea",
        "outputId": "36ff98d0-b0c8-495f-e88a-27fec005031b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to travel from Sydney to Hong Kong?\n",
            "\n",
            "Hong Kong is a city in the southern part\n",
            "45.78407335281372\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "# Now you can use the model and tokenizer for your task\n",
        "# For example, let's generate some text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs)\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d4ce0f7",
      "metadata": {
        "id": "6d4ce0f7"
      },
      "source": [
        "# GPT-J-6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad387d44",
      "metadata": {
        "id": "ad387d44"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebe2312",
      "metadata": {
        "id": "3ebe2312"
      },
      "outputs": [],
      "source": [
        "start = time()\n",
        "# Now you can use the model and tokenizer for your task\n",
        "# For example, let's generate some text\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs)\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(output_text)\n",
        "\n",
        "print(time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df181dd1",
      "metadata": {
        "id": "df181dd1"
      },
      "source": [
        "# GPT Neo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af833175",
      "metadata": {
        "id": "af833175"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the generator\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08697cf",
      "metadata": {
        "id": "c08697cf",
        "outputId": "b75404b6-e465-4582-a7b2-03ee4fcf082b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1279: UserWarning: Unfeasible length constraints: `min_length` (1000) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 50, its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to travel from Sydney to Hong Kong?\n",
            "\n",
            "You may be thinking to yourself, is it possible to travel from Sydney to Hong Kong?\n",
            "\n",
            "Yes, it is possible to travel between Sydney and Hong Kong. But what made it possible?\n",
            "9.02247953414917\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "\n",
        "input_text = \"How to travel from Sydney to Hong Kong?\"\n",
        "\n",
        "# Generate text\n",
        "output = generator(input_text, do_sample=True, min_length=1000)\n",
        "\n",
        "# Print the generated text\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "print(time() - start)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}