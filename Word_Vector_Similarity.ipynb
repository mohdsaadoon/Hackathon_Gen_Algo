{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4874f6",
      "metadata": {
        "id": "0d4874f6",
        "outputId": "dfab6849-4488-4995-f8aa-34be621c419c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
            "  \"class\": algorithms.Blowfish,\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\phykawing\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\phykawing\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords as sw\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import FastText\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7d021a",
      "metadata": {
        "id": "3a7d021a"
      },
      "source": [
        "### Define tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf7ae09",
      "metadata": {
        "id": "cdf7ae09"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "\n",
        "    # tokenized the articles into a single list\n",
        "\n",
        "    tokenized_docs = []\n",
        "\n",
        "    doc = re.sub(r'[^\\w\\s]','',text)\n",
        "\n",
        "    tokenized_doc = word_tokenize(doc)\n",
        "\n",
        "    sww = sw.words()\n",
        "    tokenized_doc = [w for w in tokenized_doc if not w in sww]\n",
        "\n",
        "    tokenized_doc = [t.lower() for t in tokenized_doc]\n",
        "\n",
        "    return tokenized_doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2fc386",
      "metadata": {
        "scrolled": false,
        "id": "3f2fc386"
      },
      "outputs": [],
      "source": [
        "question = \"Generate an approximately fifteen word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House price Range moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One\"\n",
        "\n",
        "answer = \"Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde6bd20",
      "metadata": {
        "id": "fde6bd20"
      },
      "outputs": [],
      "source": [
        "tokenized_q = set(tokenize(question))\n",
        "\n",
        "tokenized_ans = set(tokenize(answer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b37d448",
      "metadata": {
        "id": "4b37d448"
      },
      "source": [
        "### Use FastText for out of vocabulary words / misspelling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a035856a",
      "metadata": {
        "id": "a035856a"
      },
      "source": [
        "For information on pretrained FastText model:\n",
        "https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "You need to download and extract the pretrained model to the same folder of this notebook.\n",
        "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be3207b",
      "metadata": {
        "id": "2be3207b",
        "outputId": "eb9935bf-3f95-4479-dbb0-a5aad0844b47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\AppData\\Local\\Temp\\ipykernel_21384\\1446477242.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
            "  model = FastText.load_fasttext_format('cc.en.300.bin')\n"
          ]
        }
      ],
      "source": [
        "model = FastText.load_fasttext_format('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ab1ef7",
      "metadata": {
        "id": "c2ab1ef7"
      },
      "outputs": [],
      "source": [
        "def similarity_score(tokenized_q, tokenized_ans):\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for ans in tokenized_ans:\n",
        "        for q in tokenized_q:\n",
        "            q_vec = model.wv[q]\n",
        "            ans_vec = model.wv[ans]\n",
        "            score.append(model.wv.similarity(q, ans))\n",
        "\n",
        "    max_val = np.max(score)\n",
        "    std_dev_val = np.std(score)\n",
        "\n",
        "    # Filter the list for values greater than mean + std_dev\n",
        "    filtered_score = [i for i in score if i > max_val - 3 * std_dev_val]\n",
        "\n",
        "    return np.mean(filtered_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9546dc88",
      "metadata": {
        "id": "9546dc88",
        "outputId": "7647256d-8fa2-40b3-f947-dd00edf91ed9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7064993"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_score(tokenized_q, tokenized_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c889545",
      "metadata": {
        "id": "4c889545"
      },
      "source": [
        "### For example, OpenOrca Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751050ab",
      "metadata": {
        "id": "751050ab"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"Open-Orca/OpenOrca\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca2148a",
      "metadata": {
        "id": "7ca2148a",
        "outputId": "244d08da-fe89-4e02-99de-94cbae56ebaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4233923"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d860028c",
      "metadata": {
        "id": "d860028c",
        "outputId": "c78862b7-2223-4dbb-e62a-b457fe1f8dd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'For this chain-of-thought reasoning and answer, what was the question?\\nTo play on a jungle gym does not imply it is with friends.\\n A: it is not possible to tell'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[2888]['question']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2675b4",
      "metadata": {
        "id": "cb2675b4",
        "outputId": "b192523a-2b63-464c-ebcc-d847b1ccf1ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Q: Can people only have fun on a jungle gym when they're with friends?\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[2888]['response']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899c7b8d",
      "metadata": {
        "scrolled": true,
        "id": "899c7b8d",
        "outputId": "fe0b7cc6-a3ea-4e14-91b8-ce2f47bf3905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.63455343\n",
            "1 0.7215075\n",
            "2 0.5001706\n",
            "3 0.41853094\n",
            "4 0.75507814\n",
            "5 0.44518703\n",
            "6 0.5705846\n",
            "7 0.46203622\n",
            "8 0.36956504\n",
            "9 0.67863536\n",
            "10 0.7184166\n",
            "11 0.3569332\n",
            "12 0.319293\n",
            "13 0.41948196\n",
            "14 0.4369383\n",
            "15 0.64993525\n",
            "16 0.3881169\n",
            "17 0.44186518\n",
            "18 0.44386274\n",
            "19 0.45325783\n",
            "20 0.54947627\n",
            "21 0.73187596\n",
            "22 0.5714581\n",
            "23 0.42114848\n",
            "24 0.3197536\n",
            "25 0.43226904\n",
            "26 0.51541793\n",
            "27 0.527988\n",
            "28 0.6153087\n",
            "29 0.34466535\n",
            "30 0.34241438\n",
            "31 0.16293049\n",
            "32 0.44855025\n",
            "33 0.37260857\n",
            "34 0.32334316\n",
            "35 0.5296607\n",
            "36 0.723636\n",
            "37 0.39401466\n",
            "38 0.41276968\n",
            "39 0.4620061\n",
            "40 0.3958286\n",
            "41 0.4276206\n",
            "42 0.54934555\n",
            "43 0.48174268\n",
            "44 0.39043102\n",
            "45 0.47597933\n",
            "46 0.48277044\n",
            "47 0.3851124\n",
            "48 0.44256866\n",
            "49 0.6258595\n",
            "50 0.40524024\n",
            "51 0.4786667\n",
            "52 0.25549248\n",
            "53 0.4695482\n",
            "54 0.37520918\n",
            "55 0.39859876\n",
            "56 0.371849\n",
            "57 0.6719513\n",
            "58 0.5659782\n",
            "59 0.40509832\n",
            "60 0.3691469\n",
            "61 0.4300068\n",
            "62 0.43142635\n",
            "63 0.39136615\n",
            "64 0.54346895\n",
            "65 0.3335378\n",
            "66 0.40410262\n",
            "67 0.38244513\n",
            "68 0.33467835\n",
            "69 0.585711\n",
            "70 0.35310003\n",
            "71 0.43522033\n",
            "72 0.37569007\n",
            "73 0.37667918\n",
            "74 0.50622785\n",
            "75 0.44954622\n",
            "76 0.31882423\n",
            "77 0.3594118\n",
            "78 0.41174552\n",
            "79 0.3238743\n",
            "80 0.5088454\n",
            "81 0.49109036\n",
            "82 0.7005849\n",
            "83 0.30312464\n",
            "84 0.42958513\n",
            "85 0.3852932\n",
            "86 0.4793123\n",
            "87 0.72400093\n",
            "88 0.5408351\n",
            "89 0.6438911\n",
            "90 0.38965374\n",
            "91 0.36314753\n",
            "92 0.41713995\n",
            "93 0.3991629\n",
            "94 0.4065414\n",
            "95 0.49530962\n",
            "96 0.48986837\n",
            "97 0.58017427\n",
            "98 0.4159759\n",
            "99 0.33414394\n",
            "100 0.3722387\n",
            "101 0.5445517\n",
            "102 0.34463078\n",
            "103 0.4563688\n",
            "104 0.42612287\n",
            "105 0.35168663\n",
            "106 0.32621294\n",
            "107 0.6628434\n",
            "108 0.43267745\n",
            "109 0.42880043\n",
            "110 0.5670172\n",
            "111 0.42355126\n",
            "112 0.5146505\n",
            "113 0.33152625\n",
            "114 0.44019914\n",
            "115 0.5838206\n",
            "116 0.41389427\n",
            "117 0.49596867\n",
            "118 0.4299058\n",
            "119 0.43327394\n",
            "120 0.4552253\n",
            "121 0.3972735\n",
            "122 0.5144105\n",
            "123 0.38156402\n",
            "124 0.46672875\n",
            "125 0.5014921\n",
            "126 0.53829235\n",
            "127 0.79198587\n",
            "128 0.35454354\n",
            "129 0.16893418\n",
            "130 0.4110269\n",
            "131 0.5226567\n",
            "132 0.82778096\n",
            "133 1.0\n",
            "134 0.50564307\n",
            "135 1.0\n",
            "136 0.5607989\n",
            "137 0.46552703\n",
            "138 0.80253375\n",
            "139 0.73098606\n",
            "140 0.75461775\n",
            "141 0.6474948\n",
            "142 0.5273873\n",
            "143 0.27123988\n",
            "144 0.6776071\n",
            "145 0.65933573\n",
            "146 0.42790765\n",
            "147 0.62334263\n",
            "148 0.7496631\n",
            "149 0.57009256\n",
            "150 0.56203616\n",
            "151 0.5562269\n",
            "152 0.48327824\n",
            "153 0.39833158\n",
            "154 0.3160265\n",
            "155 0.51060635\n",
            "156 0.34973612\n",
            "157 0.48124856\n",
            "158 0.46392524\n",
            "159 0.55754197\n",
            "160 0.5099675\n",
            "161 0.38983902\n",
            "162 0.5714674\n",
            "163 0.32379535\n",
            "164 0.53250873\n",
            "165 0.5623597\n",
            "166 0.48605168\n",
            "167 0.4232373\n",
            "168 0.42369226\n",
            "169 0.5295118\n",
            "170 0.51689166\n",
            "171 0.33389008\n",
            "172 0.24416845\n",
            "173 0.70881706\n",
            "174 0.41968867\n",
            "175 0.5505396\n",
            "176 0.810668\n",
            "177 0.5293306\n",
            "178 0.6193767\n",
            "179 0.6938429\n",
            "180 0.37057456\n",
            "181 0.52564466\n",
            "182 0.3495515\n",
            "183 0.59641093\n",
            "184 0.33663124\n",
            "185 0.33848047\n",
            "186 0.65159845\n",
            "187 0.40875828\n",
            "188 1.0\n",
            "189 0.4851532\n",
            "190 0.5278479\n",
            "191 0.38189948\n",
            "192 0.37280607\n",
            "193 0.542105\n",
            "194 0.42633528\n",
            "195 0.37819684\n",
            "196 0.4707049\n",
            "197 0.3646913\n",
            "198 0.48206243\n",
            "199 1.0\n",
            "200 0.7505171\n",
            "201 0.4953543\n",
            "202 0.40006196\n",
            "203 0.3604707\n",
            "204 0.59938204\n",
            "205 0.43109658\n",
            "206 0.46967006\n",
            "207 0.4795449\n",
            "208 0.40472904\n",
            "209 0.38595575\n",
            "210 0.4340423\n",
            "211 0.45551044\n",
            "212 0.5159754\n",
            "213 0.555318\n",
            "214 0.35573548\n",
            "215 0.57616603\n",
            "216 0.8409547\n",
            "217 0.88444847\n",
            "218 0.7538255\n",
            "219 0.5960884\n",
            "220 0.7254616\n",
            "221 0.34854552\n",
            "222 0.4441588\n",
            "223 0.46122146\n",
            "224 0.5025198\n",
            "225 0.40293455\n",
            "226 0.19445246\n",
            "227 0.59817815\n",
            "228 0.5690127\n",
            "229 0.38630664\n",
            "230 0.37118006\n",
            "231 0.5126702\n",
            "232 0.34150207\n",
            "233\n",
            "234 0.50546026\n",
            "235 0.3701388\n",
            "236 0.6124907\n",
            "237 0.3583584\n",
            "238 0.37685224\n",
            "239 0.45592177\n",
            "240 0.32049787\n",
            "241 0.5325215\n",
            "242 0.496044\n",
            "243 0.5170191\n",
            "244 0.34234902\n",
            "245 0.60549\n",
            "246 0.4023041\n",
            "247 0.6357728\n",
            "248 0.4512932\n",
            "249 0.32956585\n",
            "250 0.76879054\n",
            "251 0.47558472\n",
            "252 0.4734688\n",
            "253 0.537434\n",
            "254 0.2879044\n",
            "255 0.38023865\n",
            "256 0.5983166\n",
            "257 0.61478204\n",
            "258 0.3335581\n",
            "259 0.5596099\n",
            "260 0.6258715\n",
            "261 0.4263805\n",
            "262 0.38646865\n",
            "263 0.5809263\n",
            "264 0.36541563\n",
            "265 0.64216727\n",
            "266 0.5377586\n",
            "267 0.51808316\n",
            "268 0.13066986\n",
            "269 0.6850227\n",
            "270 0.38690826\n",
            "271 0.638507\n",
            "272 0.5505974\n",
            "273 0.46929005\n",
            "274 0.89003354\n",
            "275 0.30184767\n",
            "276 0.5125501\n",
            "277 0.3324351\n",
            "278 0.42352968\n",
            "279 0.7860393\n",
            "280 0.41627064\n",
            "281 0.39569023\n",
            "282 0.33416837\n",
            "283 0.5157856\n",
            "284 0.38655546\n",
            "285 0.6576673\n",
            "286 0.67763305\n",
            "287 0.6573255\n",
            "288 0.5137063\n",
            "289 0.31124812\n",
            "290 0.31013846\n",
            "291 0.3665838\n",
            "292 0.29561132\n",
            "293 0.4824102\n",
            "294 0.46183836\n",
            "295 0.7115967\n",
            "296 0.5205903\n",
            "297 0.3929031\n",
            "298 0.44839957\n",
            "299 0.5313663\n",
            "300 0.55800325\n",
            "301 0.75586784\n",
            "302 0.8165928\n",
            "303 0.39411926\n",
            "304 0.7690918\n",
            "305 0.4042091\n",
            "306 0.6889596\n",
            "307 0.58019507\n",
            "308 0.44615865\n",
            "309 0.35473317\n",
            "310 0.44634852\n",
            "311 0.62186205\n",
            "312 0.19818765\n",
            "313 0.92726034\n",
            "314 0.70473605\n",
            "315 0.537817\n",
            "316\n",
            "317 0.59561986\n",
            "318 0.42582563\n",
            "319 0.50036323\n",
            "320 0.17656663\n",
            "321 0.36349526\n",
            "322 0.6528619\n",
            "323 0.72491825\n",
            "324 0.66395473\n",
            "325 0.59345573\n",
            "326 0.38395116\n",
            "327 0.33993867\n",
            "328 0.58014315\n",
            "329 0.72009665\n",
            "330 0.41925213\n",
            "331 0.5518908\n",
            "332 0.23064005\n",
            "333 0.9038429\n",
            "334 0.631541\n",
            "335 0.48672715\n",
            "336 0.68838096\n",
            "337 0.38655835\n",
            "338 0.70709807\n",
            "339 0.37408975\n",
            "340 0.33174384\n",
            "341 0.5026481\n",
            "342 0.2977351\n",
            "343 0.6675223\n",
            "344 0.77181387\n",
            "345 0.46904954\n",
            "346 0.8054034\n",
            "347 0.31125882\n",
            "348 0.5129778\n",
            "349 0.43315938\n",
            "350 0.652622\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m q \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m ans \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m tokenized_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m tokenized_ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(tokenize(ans))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenized_q) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenized_ans) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m word_tokenize(doc)\n\u001b[0;32m     11\u001b[0m sww \u001b[38;5;241m=\u001b[39m sw\u001b[38;5;241m.\u001b[39mwords()\n\u001b[1;32m---> 12\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokenized_doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m sww]\n\u001b[0;32m     14\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokenized_doc]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_doc\n",
            "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m word_tokenize(doc)\n\u001b[0;32m     11\u001b[0m sww \u001b[38;5;241m=\u001b[39m sw\u001b[38;5;241m.\u001b[39mwords()\n\u001b[1;32m---> 12\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokenized_doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msww\u001b[49m]\n\u001b[0;32m     14\u001b[0m tokenized_doc \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokenized_doc]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_doc\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "score = []\n",
        "i = 0\n",
        "\n",
        "for data in train_dataset:\n",
        "    q = data['question']\n",
        "    ans = data['response']\n",
        "\n",
        "    tokenized_q = set(tokenize(q))\n",
        "    tokenized_ans = set(tokenize(ans))\n",
        "\n",
        "    if len(tokenized_q) == 0 or len(tokenized_ans) == 0:\n",
        "        score.append(0)\n",
        "        print(i)\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    similarity = similarity_score(tokenized_q, tokenized_ans)\n",
        "\n",
        "    score.append(similarity)\n",
        "\n",
        "    print(i, similarity)\n",
        "\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1f9586",
      "metadata": {
        "id": "af1f9586"
      },
      "source": [
        "### T5-Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff313120",
      "metadata": {
        "id": "ff313120"
      },
      "outputs": [],
      "source": [
        "# Specify the model\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497f4662",
      "metadata": {
        "id": "497f4662",
        "outputId": "3e039992-e93e-488c-cd12-49b19c8872dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phykawing\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.43789995\n",
            "1 0.55974334\n",
            "2 0.27267492\n",
            "3 0.320248\n",
            "4 0.83472466\n",
            "5 0.62109196\n",
            "6 0.5776144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 0.42615637\n",
            "8 0.35319573\n",
            "9 0.2925676\n",
            "10 0.19607279\n",
            "11 0.33860868\n",
            "12 0.33148032\n",
            "13 0.6159\n",
            "14 0.3829325\n",
            "15 0.21657857\n",
            "16 0.24361515\n",
            "17 0.53316396\n",
            "18 0.43947634\n",
            "19 0.17991035\n",
            "20 0.35122898\n",
            "21 0.879655\n",
            "22 0.34119412\n",
            "23 0.17510363\n",
            "24 0.31755152\n",
            "25 0.37599966\n",
            "26 0.7518408\n",
            "27 0.640014\n",
            "28 0.17072597\n",
            "29 0.36289206\n",
            "30 0.3890201\n",
            "31 0.24465513\n",
            "32 0.3944393\n",
            "33 0.436052\n",
            "34 0.21475467\n",
            "35 0.27051073\n",
            "36 0.22590354\n",
            "37 0.27801812\n",
            "38 0.5587281\n",
            "39 0.39656395\n",
            "40 0.17036775\n",
            "41 0.25370505\n",
            "42 0.37617782\n",
            "43 0.20242378\n",
            "44 0.27483928\n",
            "45 0.7958326\n",
            "46 0.21875982\n",
            "47 0.15971427\n",
            "48 0.46482572\n",
            "49 0.36202264\n",
            "50 0.34886745\n",
            "51 0.5254971\n",
            "52 0.16075017\n",
            "53 0.2148688\n",
            "54 0.21200311\n",
            "55 0.1978098\n",
            "56 0.19609846\n",
            "57 0.39134932\n",
            "58 0.67553824\n",
            "59 0.36043847\n",
            "60 0.9334779\n",
            "61 0.546513\n",
            "62 0.42008233\n",
            "63 0.17582256\n",
            "64 0.30723855\n",
            "65 0.65156716\n",
            "66 0.18909311\n",
            "67 0.17952955\n",
            "68 0.3659422\n",
            "69 0.7199086\n",
            "70 0.37540373\n",
            "71 0.5155116\n",
            "72 0.22984813\n",
            "73 0.19197664\n",
            "74 1.0\n",
            "75 0.43008605\n",
            "76 0.32758424\n",
            "77 0.28310588\n",
            "78 0.27705634\n",
            "79 0.8865869\n",
            "80 0.38745853\n",
            "81 0.5941802\n",
            "82 0.29112983\n",
            "83 0.2291367\n",
            "84 0.30233455\n",
            "85 0.17953315\n",
            "86 0.3232246\n",
            "87 0.51817524\n",
            "88 0.18407\n",
            "89 0.7438177\n",
            "90 0.37638855\n",
            "91 0.2547709\n",
            "92 0.28518596\n",
            "93 0.25638562\n",
            "94 0.800329\n",
            "95 0.23175313\n",
            "96 0.140669\n",
            "97 0.6176095\n",
            "98 0.28185654\n",
            "99 0.3715607\n",
            "100 0.20119648\n",
            "101 0.6786117\n",
            "102 0.8446172\n",
            "103 0.24114607\n",
            "104 0.36538002\n",
            "105 0.20439562\n",
            "106 0.46230534\n",
            "107 0.2661268\n",
            "108 0.18368557\n",
            "109 0.45107114\n",
            "110 0.18888411\n",
            "111 0.14978704\n",
            "112 0.433374\n",
            "113 0.3830765\n",
            "114 0.4532919\n",
            "115 0.17693639\n",
            "116 0.40979797\n",
            "117 0.4049382\n",
            "118 0.2436758\n",
            "119 0.609607\n",
            "120 0.29609686\n",
            "121 0.29685113\n",
            "122 0.20596138\n",
            "123 0.28558022\n",
            "124 0.504372\n",
            "125 0.22139592\n",
            "126 0.13174655\n",
            "127 0.36466905\n",
            "128 0.56040674\n",
            "129 0.16547546\n",
            "130 0.2291367\n",
            "131 0.1743127\n",
            "132 0.80625355\n",
            "133 1.0\n",
            "134 0.57164526\n",
            "135 0.43652377\n",
            "136 0.6388563\n",
            "137 0.59638804\n",
            "138 0.26332605\n",
            "139 0.6784495\n",
            "140 0.80705196\n",
            "141 0.39168102\n",
            "142 0.2291367\n",
            "143 0.32204697\n",
            "144 0.1995768\n",
            "145 0.20193613\n",
            "146 0.377853\n",
            "147 0.6645091\n",
            "148 0.6203124\n",
            "149 0.2450912\n",
            "150 0.56859696\n",
            "151 0.69870704\n",
            "152 0.15186027\n",
            "153 0.296859\n",
            "154 0.22387746\n",
            "155 0.15793878\n",
            "156 0.3915326\n",
            "157 0.6002439\n",
            "158 0.89865154\n",
            "159 0.3871537\n",
            "160 0.7013298\n",
            "161 0.2476607\n",
            "162 0.32016832\n",
            "163 0.3023227\n",
            "164 0.57681686\n",
            "165 0.1532033\n",
            "166 0.2501083\n",
            "167 0.18044834\n",
            "168 0.44252762\n",
            "169 0.30690375\n",
            "170 0.20445356\n",
            "171 1.0\n",
            "172 0.20240405\n",
            "173 0.6608904\n",
            "174 0.20074213\n",
            "175 0.7041681\n",
            "176 0.9130465\n",
            "177 0.2763294\n",
            "178 0.7588041\n",
            "179 0.34848315\n",
            "180 0.31293923\n",
            "181 0.49463472\n",
            "182 0.26731488\n",
            "183 0.6599017\n",
            "184 0.36342394\n",
            "185 0.39957926\n",
            "186 0.7250971\n",
            "187 0.48389256\n",
            "188 0.38226384\n",
            "189 0.45540142\n",
            "190 0.1657896\n",
            "191 0.2904783\n",
            "192 0.21085268\n",
            "193 0.35220093\n",
            "194 0.34475684\n",
            "195 0.24156016\n",
            "196 0.39509925\n",
            "197 0.39538622\n",
            "198 0.69317406\n",
            "199 0.67484343\n",
            "200 0.8409073\n",
            "201 0.32354823\n",
            "202 0.38591418\n",
            "203 0.3963923\n",
            "204 0.3305664\n",
            "205 0.73994035\n",
            "206 0.2406305\n",
            "207 0.3386321\n",
            "208 0.19613786\n",
            "209 0.47479367\n",
            "210 0.6522085\n",
            "211 0.3478989\n",
            "212 0.16325712\n",
            "213 0.38745853\n",
            "214 0.38176742\n",
            "215 0.24569938\n",
            "216 0.83719605\n",
            "217 0.28648\n",
            "218 0.21617913\n",
            "219 0.2973591\n",
            "220 0.86672115\n",
            "221 0.25124335\n",
            "222 0.34103286\n",
            "223 0.3774033\n",
            "224 0.21402381\n",
            "225 0.35992613\n",
            "226 0.2007552\n",
            "227 0.3597392\n",
            "228 0.7137937\n",
            "229 0.29076985\n",
            "230 0.3052049\n",
            "231 0.34970242\n",
            "232 0.2006717\n",
            "233 0.22224157\n",
            "234 0.20605083\n",
            "235 0.37350935\n",
            "236 0.260884\n",
            "237 0.33618027\n",
            "238 0.25684243\n",
            "239 0.51189834\n",
            "240 0.22540328\n",
            "241 0.24821682\n",
            "242 0.48216167\n",
            "243 0.28741306\n",
            "244 0.3103442\n",
            "245 1.0\n",
            "246 0.4343991\n",
            "247 0.20715682\n",
            "248 0.5278091\n",
            "249 1.0\n",
            "250 0.5090122\n",
            "251 0.2616933\n",
            "252 0.20827003\n",
            "253 0.46680406\n",
            "254 0.22149464\n",
            "255 0.34094885\n",
            "256 0.26825193\n",
            "257 0.14098608\n",
            "258 0.43097273\n",
            "259 0.19633338\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(q, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     15\u001b[0m tokenized_ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(tokenize(output_text))\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1596\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1580\u001b[0m         input_ids,\n\u001b[0;32m   1581\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1592\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1593\u001b[0m     )\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1595\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1597\u001b[0m         input_ids,\n\u001b[0;32m   1598\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1599\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1600\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1601\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1602\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1603\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1604\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1605\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1607\u001b[0m     )\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2444\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2441\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2443\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2444\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2445\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2446\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2447\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2448\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2449\u001b[0m )\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[0;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1112\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     )\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    723\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 725\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:636\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    625\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    634\u001b[0m ):\n\u001b[0;32m    635\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 636\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    648\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:527\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[0;32m    524\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[0;32m    525\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    526\u001b[0m )\n\u001b[1;32m--> 527\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[0;32m    532\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[0;32m    533\u001b[0m     query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    534\u001b[0m )  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:502\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[1;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[0;32m    498\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(hidden_states))\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, key_length, dim_per_head)\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "score = []\n",
        "llm_ans = []\n",
        "i = 0\n",
        "\n",
        "for data in train_dataset:\n",
        "    q = data['question']\n",
        "    ans = data['response']\n",
        "\n",
        "    tokenized_q = set(tokenize(q))\n",
        "    inputs = tokenizer.encode(q, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate output\n",
        "    outputs = llm.generate(inputs)\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "    tokenized_ans = set(tokenize(output_text))\n",
        "\n",
        "    if len(tokenized_q) == 0 or len(tokenized_ans) == 0:\n",
        "        score.append(0)\n",
        "        print(i)\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    similarity = similarity_score(tokenized_q, tokenized_ans)\n",
        "\n",
        "    score.append(similarity)\n",
        "\n",
        "    llm_ans.append(output_text)\n",
        "\n",
        "    print(i, similarity)\n",
        "\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1824e67",
      "metadata": {
        "id": "c1824e67",
        "outputId": "f6ca0a61-2bf3-41ed-aa4e-1963e1233e1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Q: Answer the following question given this paragraph:   The kidneys also secrete hormones that help maintain homeostasis. For example, they produce a hormone that stimulates bone marrow to produce red blood cells when more are needed. They also secrete a hormone that regulates blood pressure and keeps it in a normal range.   Q: What organs secrete hormones that help maintain homeostasis?   A:\\nThe answer is:'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[10]['question']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2ebe46",
      "metadata": {
        "id": "5b2ebe46",
        "outputId": "284b402f-5be3-4b66-adba-fa2c0c25135a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[10]['response']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4c8f4f",
      "metadata": {
        "id": "6c4c8f4f",
        "outputId": "dec6a6e1-d2b4-49b7-9c3e-5646f6c5115a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<pad> kidneys</s>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_ans[10]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}